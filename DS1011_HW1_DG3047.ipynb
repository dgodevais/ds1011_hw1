{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 negative reviews.\n",
      "There are 12500 positive reviews.\n",
      "Loaded 25000 examples\n",
      "\n",
      "(0, \"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\")\n",
      "\n",
      "(1, \"Enchanted April is a tone poem, an impressionist painting, a masterpiece of conveying a message with few words. It has been one of my 10 favorite films since it came out. I continue to wait, albeit less patiently, for the film to come out in DVD format. Apparently, I am not alone.<br /><br />If parent company Amazon's listings are correct, there are many people who want this title in DVD format. Many people want to go to Italy with this cast and this script. Many people want to keep a permanent copy of this film in their libraries. The cast is spectacular, the cinematography and direction impeccable. The film is a definite keeper. Many have already asked. Please add our names to the list.\")\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Reading all of the negative and positive reviews into a tuple list\n",
    "neg_path = 'aclImdb/train/neg'\n",
    "pos_path = 'aclImdb/train/pos'\n",
    "negative_files = [f for f in listdir(neg_path) if isfile(join(neg_path, f))]\n",
    "postive_files = [f for f in listdir(pos_path) if isfile(join(pos_path, f))]\n",
    "\n",
    "data = []\n",
    "for neg_file in negative_files:\n",
    "    with open(neg_path + '/' + neg_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        data.append( (0,review) )\n",
    "num_neg_reviews = len(data)\n",
    "print('There are {} negative reviews.'.format(num_neg_reviews))\n",
    "for pos_file in postive_files:\n",
    "    with open(pos_path + '/' + pos_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        data.append( (1,review) )\n",
    "print('There are {} positive reviews.'.format(len(data) - num_neg_reviews))\n",
    "print('Loaded {} examples'.format(str(len(data))))\n",
    "print()\n",
    "print(data[0])\n",
    "print()\n",
    "print(data[len(data)-1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Avoid any ordering bias by randomly shuffling the list\n",
    "print([target[0] for target in data[0:10]])\n",
    "random.Random(4).shuffle(data)\n",
    "print([target[0] for target in data[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 training samples\n",
      "[0, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "5000 validation samples\n",
      "[0, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Create a validation set from the training data\n",
    "train_split = 20000\n",
    "\n",
    "train_data = data[:train_split]\n",
    "train_x = [data[1] for data in train_data]\n",
    "train_y = [data[0] for data in train_data]\n",
    "\n",
    "val_data = data[train_split:]\n",
    "val_x = [data[1] for data in val_data]\n",
    "val_y = [data[0] for data in val_data]\n",
    "\n",
    "print('{} training samples'.format(len(train_data)))\n",
    "print([target for target in train_y[0:10]])\n",
    "print('{} validation samples'.format(len(val_data)))\n",
    "print([target for target in val_y[0:10]])\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 negative test reviews.\n",
      "There are 12500 positive test reviews.\n",
      "Loaded 25000 examples\n",
      "\n",
      "(0, \"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\")\n",
      "\n",
      "(1, 'I saw this movie on TV and loved it! I am a real disaster film fan, and this one was great. The cast was made of some really interesting people. Connie Selleca is always great. And William Devane is in a league of his own. He can play both comedy and thriller in the same movie like few others can. The story line is great too. The thought of being able to follow a time line of what will happen, and to use this time line to prevent a global disaster is an interesting idea. And this movie brings it out in such a way that is almost totally believable.')\n"
     ]
    }
   ],
   "source": [
    "# Assess performance on the test set\n",
    "# Reading all of the negative and positive reviews into a tuple list\n",
    "neg_path = 'aclImdb/test/neg'\n",
    "pos_path = 'aclImdb/test/pos'\n",
    "negative_files = [f for f in listdir(neg_path) if isfile(join(neg_path, f))]\n",
    "postive_files = [f for f in listdir(pos_path) if isfile(join(pos_path, f))]\n",
    "\n",
    "test_data = []\n",
    "for neg_file in negative_files:\n",
    "    with open(neg_path + '/' + neg_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        test_data.append( (0,review) )\n",
    "num_neg_reviews = len(test_data)\n",
    "print('There are {} negative test reviews.'.format(num_neg_reviews))\n",
    "for pos_file in postive_files:\n",
    "    with open(pos_path + '/' + pos_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        test_data.append( (1,review) )\n",
    "print('There are {} positive test reviews.'.format(len(test_data) - num_neg_reviews))\n",
    "print('Loaded {} examples'.format(str(len(test_data))))\n",
    "print()\n",
    "print(test_data[0])\n",
    "print()\n",
    "print(test_data[len(test_data)-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 test samples\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_x = [data_t[1] for data_t in test_data]\n",
    "test_y = [data_t[0] for data_t in test_data]\n",
    "\n",
    "print('{} test samples'.format(len(test_data)))\n",
    "print([target for target in test_y[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "import nltk\n",
    "import os\n",
    "import pickle as pkl\n",
    "import spacy\n",
    "import string\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "class NGramBuilder(object):\n",
    "    \"\"\"\n",
    "    Class which enables hyperparameter searching over the word tokenization process\n",
    "    \"\"\"\n",
    "    # save index 0 for unk and 1 for pad\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    def __init__(self, max_vocab_size, n_gram_size, all_permutations=True):\n",
    "        \"\"\"\n",
    "        all_permutations: if True then that means if you picked an n-gram size of 3, then you would have\n",
    "        all the 1-gram and 2-gram combos included as well.\n",
    "        \"\"\"\n",
    "        self.tokenizer = spacy.load('en_core_web_sm')\n",
    "        self.punctuations = string.punctuation\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.n_gram_size = n_gram_size\n",
    "        self.all_permutations = all_permutations\n",
    "        self.vocabulary_tokens = []\n",
    "        self.id2token = None\n",
    "        self.token2id = None\n",
    "    \n",
    "    def _lower_case_remove_punc(self, parsed):\n",
    "        return [token.text.lower() for token in parsed if (token.text not in self.punctuations)]\n",
    "\n",
    "    def tokenize_dataset(self, dataset, training=False):\n",
    "        token_dataset = []\n",
    "        start_time = time.time()\n",
    "        for sample in tqdm_notebook(self.tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=4)):\n",
    "            tokens = self._lower_case_remove_punc(sample)\n",
    "            all_tokens = []\n",
    "            n_grams = nltk.ngrams(tokens, self.n_gram_size)\n",
    "            all_tokens = [' '.join(grams) for grams in n_grams]\n",
    "            token_dataset.append(all_tokens)\n",
    "            if training:\n",
    "                self.vocabulary_tokens += all_tokens\n",
    "        print(\"--- {} seconds ---\".format(time.time() - start_time))\n",
    "        return token_dataset\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        # Returns:\n",
    "        # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "        # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(self.vocabulary_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        self.id2token = list(vocab)\n",
    "        self.token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        self.id2token = ['<pad>', '<unk>'] + self.id2token\n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.vocabulary_tokens = None\n",
    "    \n",
    "    def token2index_dataset(self, tokens_data):\n",
    "        indices_data = []\n",
    "        for tokens in tokens_data:\n",
    "            index_list = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in tokens]\n",
    "            indices_data.append(index_list)\n",
    "        return indices_data\n",
    "    \n",
    "    def get_indexed_dataset_from_training_text_vector(self, text_vector, overwrite=True):\n",
    "        data_tokens = None\n",
    "        training_pickle_path = \"train_data_tokens_{}_{}.p\".format(self.n_gram_size, self.max_vocab_size)\n",
    "        id2token_path = \"id2token_{}_{}.p\".format(self.n_gram_size, self.max_vocab_size)\n",
    "        token2id_path = \"token2id_{}_{}.p\".format(self.n_gram_size, self.max_vocab_size)\n",
    "\n",
    "        train_data_tokens = None\n",
    "        if os.path.isfile(training_pickle_path) and os.path.isfile(id2token_path) and not overwrite:\n",
    "            print(\"Loading existing training token pickle file and vocabulary.\")\n",
    "            train_data_tokens = pkl.load(open(training_pickle_path, \"rb\"))\n",
    "            self.id2token = pkl.load(open(id2token_path, \"rb\"))\n",
    "            self.token2id = pkl.load(open(token2id_path, \"rb\"))\n",
    "        elif not overwrite:\n",
    "            raise ValueError('File not found for training')\n",
    "        else:\n",
    "            train_data_tokens = self.tokenize_dataset(text_vector, training=True)\n",
    "#             pkl.dump(train_data_tokens, open(training_pickle_path, \"wb\"))\n",
    "            self.build_vocab()\n",
    "#             pkl.dump(self.id2token, open(id2token_path, \"wb\"))\n",
    "#             pkl.dump(self.token2id, open(token2id_path, \"wb\"))\n",
    "\n",
    "        train_data_indices = self.token2index_dataset(train_data_tokens)\n",
    "        return train_data_indices\n",
    "        \n",
    "    \n",
    "    def get_indexed_dataset_from_val_and_test_vector(self, val_vector, test_vector, overwrite=True):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Must load training set prior to validationa and test sets')\n",
    "        data_tokens = None\n",
    "        \n",
    "        val_data_tokens = None\n",
    "        test_data_tokens = None\n",
    "        val_pickle_path = \"val_data_tokens_{}_{}.p\".format(self.n_gram_size, self.max_vocab_size)\n",
    "        test_pickle_path = \"test_data_tokens_{}_{}.p\".format(self.n_gram_size, self.max_vocab_size)\n",
    "        \n",
    "        if os.path.isfile(val_pickle_path) and os.path.isfile(test_pickle_path) and not overwrite:\n",
    "            print(\"Loading existing training token pickle file and vocabulary.\")\n",
    "            val_data_tokens = pkl.load(open(val_pickle_path, \"rb\"))\n",
    "            test_data_tokens = pkl.load(open(test_pickle_path, \"rb\"))\n",
    "        elif not overwrite:\n",
    "            raise ValueError('File not found for validation or testing')\n",
    "        else:\n",
    "            val_data_tokens = self.tokenize_dataset(val_vector)\n",
    "#             pkl.dump(val_data_tokens, open(val_pickle_path, \"wb\"))\n",
    "            test_data_tokens = self.tokenize_dataset(val_vector)\n",
    "#             pkl.dump(test_data_tokens, open(test_pickle_path, \"wb\"))\n",
    "            \n",
    "        val_data_indices = self.token2index_dataset(val_data_tokens)\n",
    "        test_data_indices = self.token2index_dataset(test_data_tokens)\n",
    "        return val_data_indices, test_data_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bag of words for 10000 vocab size and 1 gram(s)\n",
      "Building bag of words for 20000 vocab size and 1 gram(s)\n",
      "Building bag of words for 40000 vocab size and 1 gram(s)\n",
      "Building bag of words for 80000 vocab size and 1 gram(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33209436e621411290abdc3324d70ea7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f660d5b8d3df488896d9fa6d71121d87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8050d408efff4dcdaf71a48d8aad95c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab31b5d3fd9429cabf01c57ae4657fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 207.7871069908142 seconds ---\n",
      "\n",
      "--- 208.19585919380188 seconds ---\n",
      "\n",
      "--- 208.42174696922302 seconds ---\n",
      "\n",
      "--- 208.51249718666077 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889fcf8bd11c4ba1bcbecace0eaa32a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efffb669c5e9419480e0f85219f3917f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1e26c56fbe4ca387b8c8fba64b3cd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52bd46809534bc1a7f81c4152cea462"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def multi_write(vocab_size, n):\n",
    "    print(\"Building bag of words for {} vocab size and {} gram(s)\".format(vocab_size, n))\n",
    "    ngram_builder = NGramBuilder(max_vocab_size=vocab_size, n_gram_size=n, all_permutations=True) \n",
    "    train_data_indices = ngram_builder.get_indexed_dataset_from_training_text_vector(train_x)\n",
    "    val_data_indices, test_data_indices = ngram_builder.get_indexed_dataset_from_val_and_test_vector(val_x, test_x)\n",
    "\n",
    "    training_pickle_path = \"train_index_tokens_{}_{}.p\".format(n, vocab_size)\n",
    "    val_pickle_path = \"val_index_tokens_{}_{}.p\".format(n, vocab_size)\n",
    "    test_pickle_path = \"test_index_tokens_{}_{}.p\".format(n, vocab_size)\n",
    "    print('Writing the pickles')\n",
    "    pkl.dump(train_data_indices, open(training_pickle_path, \"wb\"))\n",
    "    pkl.dump(val_data_indices, open(val_pickle_path, \"wb\"))\n",
    "    pkl.dump(test_data_indices, open(test_pickle_path, \"wb\"))\n",
    "\n",
    "vocab_sizes = [10000, 20000, 40000, 80000]\n",
    "n_sizes = [1,2,3]\n",
    "# vocab_sizes = [100]\n",
    "# n_sizes = [1]\n",
    "for n in n_sizes:\n",
    "    processes = []\n",
    "    for vocab_size in vocab_sizes:\n",
    "        p = Process(target=multi_write, args=(vocab_size,n,))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for one_process in processes:\n",
    "        one_process.join()\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ngram_builder.vocabulary_tokens[0:100])\n",
    "print()\n",
    "print(train_data_indices[2])\n",
    "print()\n",
    "print(len(ngram_builder.id2token))\n",
    "#print(len(ngram_builder.vocabulary_tokens))\n",
    "print(ngram_builder.id2token[-100:])\n",
    "print()\n",
    "print([ngram_builder.token2id[k] for k in sorted(ngram_builder.token2id.keys())[:2]])\n",
    "print([v for v in list(ngram_builder.token2id.values())[:2]])\n",
    "\n",
    "print(len(train_data_indices))\n",
    "print(len(train_y))\n",
    "\n",
    "print(len(val_data_indices))\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of imdb review tokens \n",
    "        @param target_list: list of imdb review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def imdb_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding (use at least 100 to 500 and keep increasing)\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParameterTuner(object):\n",
    "    \"\"\"\n",
    "    This class will help tune hyperparameters\n",
    "    \"\"\"\n",
    "    def __init__(self,model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def get_indices_for_hyperparamters(self, n_gram_size, max_vocab_size):\n",
    "        print(\"Retrieving bag of words for {} vocab size and {} gram(s)\".format(max_vocab_size, n_gram_size))\n",
    "        training_pickle_path = \"train_index_tokens_{}_{}.p\".format(n_gram_size, max_vocab_size)\n",
    "        val_pickle_path = \"val_index_tokens_{}_{}.p\".format(n_gram_size, max_vocab_size)\n",
    "        test_pickle_path = \"test_index_tokens_{}_{}.p\".format(n_gram_size, max_vocab_size)\n",
    "        train_data_indices = pkl.load(open(training_pickle_path, \"rb\"))\n",
    "        val_data_indices = pkl.load(open(val_pickle_path, \"rb\"))\n",
    "        test_data_indices = pkl.load(open(test_pickle_path, \"rb\"))\n",
    "        return train_data_indices, val_data_indices, test_data_indices\n",
    "\n",
    "    # Function for testing the model\n",
    "    def test_model2(self, loader):\n",
    "        \"\"\"\n",
    "        Help function that tests the model's performance on a dataset\n",
    "        @param: loader - data loader for the dataset to test against\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        self.model.eval()\n",
    "        for data, lengths, labels in loader:\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            outputs = F.softmax(self.model(data_batch, length_batch), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total)\n",
    "\n",
    "    def train_and_validate_model(self, num_epochs, train_loader, val_loader):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            last_accuracy = 0\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                self.model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                # validate every 100 iterations\n",
    "                if i > 0 and i % 100 == 0:\n",
    "                    # validate\n",
    "                    print('loss: ' + str(running_loss))\n",
    "                    val_acc = self.test_model2(val_loader)\n",
    "                    last_accuracy = val_acc\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                               epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            losses.append(running_loss)\n",
    "            accuracies.append(last_accuracy)\n",
    "        print( (accuracies, losses))\n",
    "        return accuracies, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.01\n",
      "Retrieving bag of words for 10000 vocab size and 1 gram(s)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3471ed74633f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuning_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tuning_record_{}_{}.p\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# vocab_sizes = [10000, 20000, 40000, 80000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3471ed74633f>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(max_vocab_size, n)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImdbDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_y' is not defined"
     ]
    }
   ],
   "source": [
    "def run_model(max_vocab_size,n):\n",
    "    tuning_record = []\n",
    "#     learning_rates = [0.5, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "    learning_rates = [0.01]\n",
    "    emb_dim = 100\n",
    "    model = BagOfWords(max_vocab_size + 2, emb_dim)\n",
    "    for learning_rate in learning_rates:\n",
    "        print(\"learning rate: \"+ str(learning_rate))\n",
    "        # Criterion and Optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss() \n",
    "        # criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        hpt = HyperParameterTuner(model, criterion, optimizer)\n",
    "        train_data_indices, val_data_indices, test_data_indices = hpt.get_indices_for_hyperparamters(n, max_vocab_size)\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "        print(len(train_y))\n",
    "        print(len(train_data_indices))\n",
    "        train_dataset = ImdbDataset(train_data_indices, train_y)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_collate_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = ImdbDataset(val_data_indices, val_y)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_collate_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        num_epochs = 20 # number epoch to train\n",
    "        accuracies, losses = hpt.train_and_validate_model(num_epochs, train_loader, val_loader)\n",
    "        accuracies_all = (max_vocab_size, n, learning_rate, accuracies, losses)\n",
    "        tuning_record.append(accuracies_all)\n",
    "        print(accuracies_all)\n",
    "    pkl.dump(tuning_record, open(\"tuning_record_{}_{}.p\".format(n, max_vocab_size), \"wb\"))\n",
    "\n",
    "run_model(10000,1)\n",
    "    \n",
    "# vocab_sizes = [10000, 20000, 40000, 80000]\n",
    "# n_sizes = [1,2,3]\n",
    "# for n in n_sizes:\n",
    "#     processes = []\n",
    "#     for vocab_size in vocab_sizes:\n",
    "#         p = Process(target=run_model, args=(vocab_size,n,))\n",
    "#         processes.append(p)\n",
    "#         p.start()\n",
    "\n",
    "#     for one_process in processes:\n",
    "#         one_process.join()\n",
    "\n",
    "#     print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_record2 = pkl.load(open(\"tuning_record.p\", \"rb\"))\n",
    "print(str(tuning_record2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = 12, 12\n",
    "\n",
    "val_predicted_y = []\n",
    "actual_y = []\n",
    "\n",
    "for data, lengths, labels in val_loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        print(model(data_batch, length_batch))\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        print(outputs[:,0])\n",
    "#         val_predicted_y += (outputs.max(1, keepdim=True)[1]).squeeze(1).tolist()\n",
    "        val_predicted_y += outputs[:,0].squeeze(0).tolist()\n",
    "        actual_y += labels.tolist()\n",
    "        print(val_predicted_y)\n",
    "        print(actual_y)\n",
    "        break\n",
    "\n",
    "fpr_log_ct, tpr_log_ct, threshold_log_ct = roc_curve(actual_y, val_predicted_y)\n",
    "roc_auc_log_ct = auc(fpr_log_ct, tpr_log_ct)\n",
    "print(roc_auc_log_ct)\n",
    "\n",
    "plt.title('Comparing ROC Across Different Vocabulary Sizes')\n",
    "plt.plot(fpr_log_ct, tpr_log_ct, 'b', label = 'AUC for vocab size of 10,000 = %0.4f' % roc_auc_log_ct)\n",
    "# plt.plot(fpr_nb_ct, tpr_nb_ct, 'b', label = 'AUC for vocab size of 20,000 = %0.4f' % roc_auc_nb_ct, color='purple')\n",
    "# plt.plot(fpr_log_tfidf, tpr_log_tfidf, 'b', label = 'AUC for vocab size of 40,000 = %0.4f' % roc_auc_log_tfidf, color='green')\n",
    "# plt.plot(fpr_nb_tfidf, tpr_nb_tfidf, 'b', label = 'AUC for vocab size of 80,000 = %0.4f' % roc_auc_nb_tfidf, color='orange')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess performance on the test set\n",
    "# Reading all of the negative and positive reviews into a tuple list\n",
    "neg_path = 'aclImdb/test/neg'\n",
    "pos_path = 'aclImdb/test/pos'\n",
    "negative_files = [f for f in listdir(neg_path) if isfile(join(neg_path, f))]\n",
    "postive_files = [f for f in listdir(pos_path) if isfile(join(pos_path, f))]\n",
    "\n",
    "test_data = []\n",
    "for neg_file in negative_files:\n",
    "    with open(neg_path + '/' + neg_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        test_data.append( (0,review) )\n",
    "num_neg_reviews = len(test_data)\n",
    "print('There are {} negative test reviews.'.format(num_neg_reviews))\n",
    "for pos_file in postive_files:\n",
    "    with open(pos_path + '/' + pos_file, 'r') as f:\n",
    "        review = f.read()\n",
    "        test_data.append( (1,review) )\n",
    "print('There are {} positive test reviews.'.format(len(test_data) - num_neg_reviews))\n",
    "print('Loaded {} examples'.format(str(len(test_data))))\n",
    "print()\n",
    "print(test_data[0])\n",
    "print()\n",
    "print(test_data[len(test_data)-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set from the training data\n",
    "test_x = [data_t[1] for data_t in test_data]\n",
    "test_y = [data_t[0] for data_t in test_data]\n",
    "\n",
    "print('{} test samples'.format(len(test_data)))\n",
    "print([target for target in test_y[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_x)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "test_dataset = ImdbDataset(test_data_indices, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_log_ct, tpr_log_ct, threshold_log_ct = roc_curve(Y_test, preds_log_ct)\n",
    "roc_auc_log_ct = auc(fpr_log_ct, tpr_log_ct)\n",
    "\n",
    "plt.title('ROC For Test Set With Best Model')\n",
    "plt.plot(fpr_nb_tfidf, tpr_nb_tfidf, 'b', label = 'AUC for tfidf Naive Bayes = %0.4f' % roc_auc_nb_tfidf, color='orange')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
